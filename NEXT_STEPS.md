# üî• PROCHAINES √âTAPES - Plan D√©taill√©

**Date:** 27/12/2025  
**Statut:** üöß Phase ETL 1 termin√©e - 2/3 modules pr√™ts

Le document **Plan B iSaVigne** te fournit la strat√©gie compl√®te. Voici les √©tapes concr√®tes √† faire maintenant, dans l'ordre.

---

## üöß √âTAPE 1: Finaliser le Pipeline ETL (Jeudi - Vendredi)

### 1.1 Cr√©er `load_postgres.py` (Chargement en BD)

**Objectif:** Charger les donn√©es CURATED transform√©es dans PostgreSQL.

**√Ä faire:**
```python
# etl/load_postgres.py

import pandas as pd
from sqlalchemy import create_engine
from etl.config import DATABASE_URL, logger

def load_table(table_name, csv_file):
    """
    Charge un fichier CSV dans une table PostgreSQL
    
    Args:
        table_name: nom de la table cible (ex: 'etl.ventes_lignes')
        csv_file: chemin du fichier CURATED
    
    Returns:
        dict avec statut de chargement
    """
    try:
        # Charger le CSV
        df = pd.read_csv(csv_file, dtype={'client_code': str, 'produit_key': str})
        
        # Connexion PostgreSQL
        engine = create_engine(DATABASE_URL)
        
        # D√©tection des doublons par cl√© naturelle
        # Pour ventes: (document_id, produit_key, client_code)
        if table_name == 'etl.ventes_lignes':
            key_cols = ['document_id', 'produit_key', 'client_code']
            df_dedup = df.drop_duplicates(subset=key_cols, keep='last')
            nb_dupes = len(df) - len(df_dedup)
            if nb_dupes > 0:
                logger.warning(f"D√©tect√© {nb_dupes} doublons sur {table_name}")
            df = df_dedup
        
        # Chargement
        df.to_sql(
            table_name.split('.')[1],  # nom table
            engine,
            schema=table_name.split('.')[0],  # sch√©ma
            if_exists='append',  # ajouter, ne pas remplacer
            index=False,
            chunksize=500  # par lots
        )
        
        logger.info(f"‚úì Charg√© {len(df)} lignes dans {table_name}")
        return {'success': True, 'rows_loaded': len(df), 'duplicates_removed': nb_dupes}
        
    except Exception as e:
        logger.error(f"‚úó Erreur chargement {table_name}: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_all_curated():
    """
    Charge tous les fichiers curated en PostgreSQL
    """
    from pathlib import Path
    from etl.config import CURATED_DIR
    
    results = {}
    curated_files = list(CURATED_DIR.glob('*.csv'))
    
    for csv_file in curated_files:
        # D√©terminer le nom de la table √† partir du nom du fichier
        if 'VENTES_LIGNES' in csv_file.name:
            table_name = 'etl.ventes_lignes'
        elif 'CLIENTS' in csv_file.name:
            table_name = 'etl.clients'
        elif 'PRODUITS' in csv_file.name:
            table_name = 'etl.produits'
        elif 'STOCK' in csv_file.name:
            table_name = 'etl.stock'
        else:
            logger.warning(f"Fichier curated non reconnu: {csv_file.name}")
            continue
        
        results[table_name] = load_table(table_name, str(csv_file))
    
    return results

if __name__ == '__main__':
    logger.info("=== CHARGEMENT CURATED ‚Üí PostgreSQL ===")
    results = load_all_curated()
    for table, result in results.items():
        if result['success']:
            print(f"‚úì {table}: {result['rows_loaded']} lignes")
        else:
            print(f"‚úó {table}: {result['error']}")
```

### 1.2 Cr√©er `main.py` (Orchestration)

**Objectif:** Orchestrer tout le pipeline (ingest ‚Üí transform ‚Üí load) en une seule commande.

**√Ä faire:**
```python
# etl/main.py

import time
from datetime import datetime
from etl.config import logger
from etl.ingest_raw import ingest_all_datasets
from etl.transform_sales import process_all_sales_files
from etl.load_postgres import load_all_curated

def run_etl_pipeline():
    """
    Orchestre le pipeline ETL complet:
    1. Ingestion RAW ‚Üí STAGING
    2. Transformation STAGING ‚Üí CURATED
    3. Chargement CURATED ‚Üí PostgreSQL
    """
    start_time = time.time()
    
    try:
        logger.info("\n" + "="*60)
        logger.info("üìä D√âMARRAGE PIPELINE ETL COMPLET")
        logger.info(f"Date/Heure: {datetime.now().isoformat()}")
        logger.info("="*60 + "\n")
        
        # √âTAPE 1: Ingestion
        logger.info("\nüîµ √âTAPE 1/3: INGESTION RAW ‚Üí STAGING")
        ingest_results = ingest_all_datasets()
        logger.info(f"R√©sultats ingestion: {ingest_results}")
        
        # √âTAPE 2: Transformation
        logger.info("\nüîµ √âTAPE 2/3: TRANSFORMATION STAGING ‚Üí CURATED")
        transform_results = process_all_sales_files()
        logger.info(f"R√©sultats transformation: {transform_results}")
        
        # √âTAPE 3: Chargement
        logger.info("\nüîµ √âTAPE 3/3: CHARGEMENT CURATED ‚Üí PostgreSQL")
        load_results = load_all_curated()
        logger.info(f"R√©sultats chargement: {load_results}")
        
        # R√©sum√© final
        duration = time.time() - start_time
        logger.info("\n" + "="*60)
        logger.info(f"‚úÖ SUCC√àS COMPLET en {duration:.2f}s")
        logger.info("="*60 + "\n")
        
        return {
            'success': True,
            'duration': duration,
            'ingest': ingest_results,
            'transform': transform_results,
            'load': load_results
        }
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"\n‚ùå ERREUR PIPELINE apr√®s {duration:.2f}s: {str(e)}", exc_info=True)
        return {
            'success': False,
            'duration': duration,
            'error': str(e)
        }

if __name__ == '__main__':
    result = run_etl_pipeline()
    exit(0 if result['success'] else 1)
```

### 1.3 Tester avec Donn√©es de Test

**Cr√©er les fichiers de test:**

```bash
# Dossiers
mkdir -p "C:\Users\Valentin\Desktop\CRM_Ruhlmann\exports\raw\isavigne\{ventes_lignes,clients,produits,stock}"

# Cr√©er ventes_lignes_2025-12-27.csv
echo "client_code,date_livraison,produit_label,qty_line,pu_ht,mt_ht,mt_ttc,marge,document_type,document_no,article,email,code_postal,ville" > ventes_lignes_2025-12-27.csv
echo "CL001,27/12/2025,Cremant Alsace Extra Brut,1,8.5,8.5,10.2,2.0,VENTE,V0001,CREMANT,jean@test.fr,67000,Strasbourg" >> ventes_lignes_2025-12-27.csv
echo "CL002,27/12/2025,Gewurztraminer VT,2,15.0,30.0,36.0,8.0,VENTE,V0002,GEWURZ,marie@test.fr,75000,Paris" >> ventes_lignes_2025-12-27.csv
```

**Ex√©cuter le pipeline:**

```bash
cd C:\Windows\System32\crm-reco-platform
python etl/main.py
```

**V√©rifier les r√©sultats:**

```bash
# V√©rifier les fichiers g√©n√©r√©s
dir exports\staging\        # fichiers horodat√©s
dir exports\curated\        # fichiers transform√©s
dir exports\logs\           # logs d'ex√©cution

# V√©rifier la base PostgreSQL
docker exec crm-postgres psql -U crm_user -d crm_reco -c "SELECT COUNT(*) FROM etl.ventes_lignes;"
```

---

## üöß √âTAPE 2: Int√©gration Brevo (Lundi - Mercredi)

### 2.1 Module de Synchronisation Brevo

**Objectif:** Envoyer les recommandations via email via la plateforme Brevo (anciennement Sendinblue).

**√Ä cr√©er:** `etl/brevo_integration.py`

**Fonctionnalit√©s:**
- Authentification API Brevo
- Upload contacts
- Envoi emails personnalis√©s
- Log des statuts (ok, bounce, opt-out)

### 2.2 Templates d'Emails

**√Ä cr√©er:**
- Template rebuy (racheter un produit achet√©)
- Template cross-sell (produit compl√©mentaire)
- Template winback (r√©activer client inactif)

### 2.3 Scheduleur d'Envois

**√Ä cr√©er:** D√©clencher les envois √† partir du dashboard avec validation.

---

## üöß √âTAPE 3: Moteur de Recommandations (Mercredi - Jeudi)

### 3.1 Analyse RFM

**√Ä cr√©er:** Scoring bas√© sur Recency, Frequency, Monetary Value.

**Table r√©sultante:** `crm.rfm_scores`

### 3.2 Scoring Co-achats

**√Ä cr√©er:** Identifier les paires de produits achet√©es ensemble (cross-sell).

### 3.3 Recommendations Candidate

**√Ä cr√©er:** Table `crm.recommendations` avec produits propos√©s et score.

---

## üöß √âTAPE 4: Automatisation (Power Automate Desktop)

### 4.1 Cr√©er Flow PAD pour Exports iSaVigne

**Objectif:** Automatiser les exports depuis iSaVigne vers le dossier RAW.

**Proc√©dure:**
1. Ouvrir Power Automate Desktop
2. Cr√©er flow: "EXPORT_ISAVIGNE_HEBDO"
3. Enregistrer les √©tapes manuellement
4. Planifier via Planificateur de t√¢ches Windows

### 4.2 Planifier Ex√©cution

**Planificateur de t√¢ches:**
- **Jour:** Lundi
- **Heure:** 06:00
- **Actions:**
  1. Exports iSaVigne (PAD)
  2. Lancer pipeline ETL (python etl/main.py)
  3. G√©n√©rer recommandations
  4. Envoyer emails (Brevo)

---

## üéØ D√âPLOIEMENT VPS OVH (Semaine prochaine)

### 5.1 Pr√©paration

- [ ] Commander VPS OVH (2-4 vCPU, 4-8GB RAM, 40GB SSD)
- [ ] Setup Ubuntu 22.04 LTS
- [ ] Installer Docker & Docker Compose
- [ ] Domaine personnalis√© (ex: crm.ruhlmann.fr)
- [ ] SSL/HTTPS via Let's Encrypt

### 5.2 D√©ploiement

- [ ] Pusher code sur GitHub
- [ ] Cloner sur VPS
- [ ] Lancer `docker-compose up -d`
- [ ] Configurer nginx reverse proxy
- [ ] Tests d'acc√®s

### 5.3 Production Readiness

- [ ] Sauvegardes base PostgreSQL (quotidiennes)
- [ ] Monitoring uptime
- [ ] Logs centralis√©s
- [ ] Plan de r√©cup√©ration catastrophe

---

## ‚úÖ Checklist Cette Semaine

### Jeudi (Aujourd'hui/Demain)

- [ ] `load_postgres.py` √©crit et test√©
- [ ] `main.py` √©crit et test√©
- [ ] Pipeline ETL complet fonctionnel (ingest ‚Üí transform ‚Üí load)
- [ ] Donn√©es de test charg√©es en PostgreSQL
- [ ] Logs v√©rifi√©s (pas d'erreur)

### Vendredi

- [ ] Tester avec donn√©es r√©elles iSaVigne (petit extrait si possible)
- [ ] Valider la qualit√© des transformations
- [ ] Documenter les anomalies d√©couvertes
- [ ] Pr√©parer rapport "Data Health" pour le dashboard

### Lundi (Semaine prochaine)

- [ ] D√©marrer int√©gration Brevo
- [ ] Cr√©er templates d'emails
- [ ] Commencer le moteur de recommandations

---

## üìÑ Fichiers √† Cr√©er/Modifier

### √Ä cr√©er:
- ‚úÖ `etl/config.py` ‚úì FAIT
- ‚úÖ `etl/normalizers.py` ‚úì FAIT
- ‚úÖ `etl/ingest_raw.py` ‚úì FAIT
- ‚úÖ `etl/transform_sales.py` ‚úì FAIT
- ‚úÖ `etl/create_schema.sql` ‚úì FAIT
- üî¥ `etl/load_postgres.py` ‚Üê **√Ä FAIRE IMM√âDIATEMENT**
- üî¥ `etl/main.py` ‚Üê **√Ä FAIRE IMM√âDIATEMENT**

### √Ä cr√©er apr√®s:
- `etl/brevo_integration.py`
- `etl/recommendations_engine.py`
- `etl/quality_checks.py` (optionnel mais recommand√©)
- `backend/routes/recommendations.py` (endpoint API)

---

## üìà Architecture Finale (Semaine 2)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       iSaVigne (source)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Power Automate Desktop   ‚îÇ (RPA)
       ‚îÇ Flow: EXPORT_ISAVIGNE    ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
           Lundi 06:00 (auto)
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Exports CSV/XLSX         ‚îÇ
       ‚îÇ ‚Üí RAW folder             ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ ETL Pipeline (Python)    ‚îÇ
       ‚îÇ  1. Ingest              ‚îÇ
       ‚îÇ  2. Transform           ‚îÇ
       ‚îÇ  3. Load PostgreSQL     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ PostgreSQL Warehouse     ‚îÇ
       ‚îÇ - etl.ventes_lignes     ‚îÇ
       ‚îÇ - etl.clients           ‚îÇ
       ‚îÇ - etl.produits          ‚îÇ
       ‚îÇ - crm.customer_360      ‚îÇ
       ‚îÇ - crm.recommendations   ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Recommendation Engine    ‚îÇ
       ‚îÇ (RFM + Cross-sell)       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Brevo Integration        ‚îÇ
       ‚îÇ (Email campaigns)        ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Frontend Dashboard       ‚îÇ
       ‚îÇ - Data Health           ‚îÇ
       ‚îÇ - Recommendations       ‚îÇ
       ‚îÇ - Approvals            ‚îÇ
       ‚îÇ - Compliance           ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù Notes Importantes

1. **Sans API iSaVigne:** Le plan B (exports + RPA) est robuste et scalable.

2. **Qualit√© > Vitesse:** Les erreurs de donn√©es entra√Ænent de mauvaises recommandations.

3. **Automatisation:** Une fois configur√©e, tout tourne tous les lundis sans intervention.

4. **Incr√©mental:** Ne pas recharger les 5 ans d'historique, seulement les nouveaut√©s.

5. **Tra√ßabilit√©:** Les logs permettent de retracer chaque ligne ‚Üí tr√®s utile en d√©bug.

---

## üîó Ressources

- **Plan Complet:** [Plan B iSaVigne](file:5)
- **Documentation ETL:** [ETL_README.md](ETL_README.md)
- **Quick Start:** [GETTING_STARTED.md](GETTING_STARTED.md)
- **Status Projet:** [PROJET_STATUS.md](PROJET_STATUS.md)

---

**Derni√®re mise √† jour:** 27/12/2025 16:25 CET  
**Prochaine √©tape:** Cr√©er `load_postgres.py` et `main.py`  
**Deadline:** Jeudi 28/12/2025
